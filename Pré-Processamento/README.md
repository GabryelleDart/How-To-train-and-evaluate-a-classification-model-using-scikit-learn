# Preparar e prÃ©-processar um dataset pequeno para experimentaÃ§Ã£o
> A tarefa consiste em pegar um conjunto de dados pequeno e deixÃ¡-lo "arrumado" para que o computador consiga aprender com ele.

O prÃ©- processamento consistiria em:

- **Organizar os dados** â†’ tirar duplicados, completar onde falta informaÃ§Ã£o ou decidir jogar fora dados incompletos.


- **Traduzir informaÃ§Ãµes em nÃºmeros** â†’ o computador entende melhor nÃºmeros do que palavras.
    - Ex: transformar â€œMasculino/Femininoâ€ em `0` e `1`.
- **Colocar tudo na mesma escala** â†’ se uma coluna tem valores como â€œidade = 25â€ e outra â€œsalÃ¡rio = 8000â€, o modelo pode dar mais importÃ¢ncia pro salÃ¡rio sÃ³ porque Ã© um nÃºmero maior. A normalizaÃ§Ã£o serve pra â€œequilibrarâ€ isso.
- **Separar treino e teste** â†’ Ã© como estudar para uma prova: vocÃª aprende com um pedaÃ§o do material (treino) e depois se testa com outro pedaÃ§o (teste) para ver se realmente aprendeu.

EntÃ£o, prÃ©-processar um dataset pequeno serve para limpar, organizar e transformar os dados em um formato que o modelo de inteligÃªncia artificial consiga entender e aprender de verdade.
Assim, quando vocÃª for treinar o modelo (tipo uma Ã¡rvore de decisÃ£o, uma regressÃ£o ou uma rede simples), ele nÃ£o vai se confundir com valores faltando, categorias em texto ou nÃºmeros em escalas muito diferentes.
### PrÃ©- requisitos para essa tarefa:
- [x]  Python instalado

```jsx
python --version

```

- [x]  Bibliotecas :
- `numpy` (cÃ¡lculos numÃ©ricos)
- `pandas` (manipulaÃ§Ã£o de dados)
- `matplotlib` / `seaborn` (visualizaÃ§Ã£o)
- `scikit-learn` (modelos de machine learning)

```jsx
python -m pip show scikit-learn
//caso nao apareÃ§a baixe usando 
conda install scikit-learn
//ou 
pip install scikit-learn

```

- [x]  Editor para rodar os CÃ³digos ( Jupiter Notebook)
- [x]  Conhecimentos bÃ¡sicos de Python (mÃ­nimo necessÃ¡rio)
### Realizando a tarefa:
#### ğŸš¦ Passo a passo:
1. Carregar o dataset (ex.: Iris, CSV etc.).
```
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df['target'] = iris.target
```
2. Inspecionar os dados (ver tamanho, tipos, valores faltantes, classes).
```
# tamanho (linhas, colunas)
print("Shape:", df.shape)

# primeiras linhas
print(df.head())

# tipos de dados
print(df.info())

# valores faltantes
print("Valores faltantes por coluna:\n", df.isnull().sum())

# distribuiÃ§Ã£o do target
print("Classes disponÃ­veis:", df['target'].unique())
print("Contagem por classe:\n", df['target'].value_counts())
```
3. Limpar/transformar os dados (tratar valores nulos, remover colunas inÃºteis, ajustar formatos).
```
# exemplo: remover colunas inÃºteis (nÃ£o necessÃ¡rio no Iris)
# df = df.drop(columns=['coluna_irrelevante'])

# exemplo: tratar valores nulos
df = df.fillna(df.mean(numeric_only=True))  # preenche nulos com mÃ©dia
```
4. Dividir em features (X) e target (y) (o que entra no modelo e o que queremos prever).
```
# X = dados de entrada (features)
X = df.drop('target', axis=1)

# y = o que queremos prever (rÃ³tulo / classe)
y = df['target']

print("Features (X):")
print(X.head())
print("\nTarget (y):")
print(y.head())
```
5. Separar treino e teste (train_test_split) â€” muito importante para garantir que o modelo seja avaliado de forma justa.
```
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Tamanho treino:", X_train.shape)
print("Tamanho teste:", X_test.shape)
```
6. Aplicar transformaÃ§Ãµes:
- 6.1 ImputaÃ§Ã£o de valores faltantes
```
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

```
- 6.2 NormalizaÃ§Ã£o / PadronizaÃ§Ã£o
```
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)
```
- 6.3 CodificaÃ§Ã£o de variÃ¡veis categÃ³ricas (se existissem)
```
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
categorical_data = [['vermelho'], ['azul'], ['verde']]
encoded = encoder.fit_transform(categorical_data)

print(encoded)  # vira nÃºmeros binÃ¡rios (one-hot)

```

#### âœ… Forma 1 â€” Usando um dataset nativo do Scikit-learn

O Scikit-learn jÃ¡ traz datasets pequenos para prÃ¡tica (Iris, Breast Cancer, Digits, Wine...).

No arquivo   tem a demostraÃ§Ã£o da preparaÃ§Ã£o e prÃ©-processamento do Dataset Iris `(conjunto de dados que  consiste em 50 amostras de cada uma das trÃªs espÃ©cies de Iris ( Iris setosa, Iris virginica e Iris versicolor)`.
<img width="1898" height="823" alt="image" src="https://github.com/user-attachments/assets/a0407a65-35bf-4bdf-a7a7-0e9a508dd772" />


#### âœ… Forma 2 â€” Usando um dataset de CSV (ex.: baixado da internet)
> ##### ğŸ”¹  Onde conseguir datasets pequenos
> Aqui alguns lugares Ã³timos:
> 1. **Nativos no Scikit-learn** â†’ Iris, Wine, Breast Cancer, Digits.
> 2. **Seaborn** â†’ vem com vÃ¡rios datasets prontos (`sns.load_dataset("tips")`).
> 3. **Kaggle** â†’ plataforma com milhares de datasets (precisa conta gratuita). `https://www.kaggle.com/datasets`
> 4. **UCI Machine Learning Repository** â†’ datasets clÃ¡ssicos. ` https://archive.ics.uci.edu/ml/index.php`
> 5. **GitHub** â†’ vÃ¡rios repositÃ³rios com datasets em CSV.

#### âœ… Forma 3 â€” Criando um dataset aleatÃ³rio (Ãºtil para teste rÃ¡pido)
VocÃª pode gerar dados fictÃ­cios para treinar um modelo.

---
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br> 

## 2Â° task: Avaliar rapidamente o desempenho do modelo treinado
### ğŸ”¹ O que significa avaliar o desempenho?
### ğŸ”¹Por que precisamos avaliar?
### ğŸ”¹ Para que serve?
### ğŸ”¹ Formas de avaliar modelos de classificaÃ§Ã£o:
### ğŸ”¹ O que define se foi bem avaliado ou nÃ£o?
